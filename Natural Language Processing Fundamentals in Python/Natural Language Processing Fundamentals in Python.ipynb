{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Which pattern?\n",
    "50xp\n",
    "Which of the following Regex patterns result in the following text?\n",
    "\n",
    "`>>> my_string = \"Let's write RegEx!\"\n",
    "`>>> re.findall(PATTERN, my_string)`\n",
    "['Let', 's', 'write', 'RegEx']`\n",
    "\n",
    "Possible Answers\n",
    "Click or Press Ctrl+1 to focus\n",
    "PATTERN = r\"\\s+\"\n",
    "PATTERN = r\"\\w+\" (Correct)\n",
    "PATTERN = r\"[a-z]\"\n",
    "PATTERN = r\"\\w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 's', 'write', 'RegEx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Let's write RegEx!\"\n",
    "PATTERN = r\"\\w+\" \n",
    "re.findall(PATTERN, my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practicing regular expressions: re.split() and re.findall()\n",
    "\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"/n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as \"/n\".\n",
    "\n",
    "Instructions\n",
    " - Import the regular expression module re.\n",
    " - Split my_string on each sentence ending. To do this:\n",
    "     - Write a pattern called sentence_endings to match sentence endings (., !, and ?).\n",
    "     - Use re.split() to split my_string on the pattern and print the result.\n",
    " - Find all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall(). Print the result.\n",
    " - Write a pattern called spaces to match spaces and then use re.split() to split my_string on this pattern, keeping all punctuation intact. Print the result.\n",
    " - Find all digits in my_string by writing a pattern called digits and using re.findall(). Print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.!?]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'RegEx', \"Won't\", 'Can', 'Or']\n"
     ]
    }
   ],
   "source": [
    "capitalized_words = r\"[A-Z][\\w']+\"\n",
    "print(re.findall(capitalized_words, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to tokenization - Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization with NLTK\n",
    "\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. You will have access to word_tokenize and sent_tokenize from the nltk.tokenize library. You can utilize them to tokenize both words and sentences from Python strings.\n",
    "\n",
    "Instructions\n",
    " - Import the sent_tokenize and word_tokenize functions from nltk.tokenize.\n",
    " - Tokenize all the sentences in scene_one.\n",
    " - Tokenize the fourth sentence in sentences.\n",
    " - Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set.\n",
    " - Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sent_tokenize and word_tokenize from nltk.tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with re.search()\n",
    "\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.\n",
    "\n",
    "Instructions\n",
    " - Print the start and end indexes for the first occurence of the word \"coconuts\" in scene_one. To do this, first use re.search() to define match, and then use the .start() and .end() methods of match.\n",
    " - Write a regular expression called pattern1 to find anything in square brackets.\n",
    " - Use re.search() with the previous pattern to find the first text in square brackets in the scene.\n",
    " - Use re.match() to match the script notation in the fourth line (ARTHUR:) and print the result. The tokenized sentences of scene_one are available in your namespace as sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!',\n",
    " '[clop clop clop] \\nSOLDIER #1: Halt!',\n",
    " 'Who goes there?',\n",
    " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.',\n",
    " 'King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
    " 'SOLDIER #1: Pull the other one!',\n",
    " 'ARTHUR: I am, ...  and this is my trusty servant Patsy.',\n",
    " 'We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.',\n",
    " 'I must speak with your lord and master.',\n",
    " 'SOLDIER #1: What?',\n",
    " 'Ridden on a horse?',\n",
    " 'ARTHUR: Yes!',\n",
    " \"SOLDIER #1: You're using coconuts!\",\n",
    " 'ARTHUR: What?',\n",
    " \"SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\",\n",
    " 'ARTHUR: So?',\n",
    " \"We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\",\n",
    " 'ARTHUR: We found them.',\n",
    " 'SOLDIER #1: Found them?',\n",
    " 'In Mercea?',\n",
    " \"The coconut's tropical!\",\n",
    " 'ARTHUR: What do you mean?',\n",
    " 'SOLDIER #1: Well, this is a temperate zone.',\n",
    " 'ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?',\n",
    " 'SOLDIER #1: Are you suggesting coconuts migrate?',\n",
    " 'ARTHUR: Not at all.',\n",
    " 'They could be carried.',\n",
    " 'SOLDIER #1: What?',\n",
    " 'A swallow carrying a coconut?',\n",
    " 'ARTHUR: It could grip it by the husk!',\n",
    " \"SOLDIER #1: It's not a question of where he grips it!\",\n",
    " \"It's a simple question of weight ratios!\",\n",
    " 'A five ounce bird could not carry a one pound coconut.',\n",
    " \"ARTHUR: Well, it doesn't matter.\",\n",
    " 'Will you go and tell your master that Arthur from the Court of Camelot is here.',\n",
    " 'SOLDIER #1: Listen.',\n",
    " 'In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?',\n",
    " 'ARTHUR: Please!',\n",
    " 'SOLDIER #1: Am I right?',\n",
    " \"ARTHUR: I'm not interested!\",\n",
    " 'SOLDIER #2: It could be carried by an African swallow!',\n",
    " 'SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.',\n",
    " \"That's my point.\",\n",
    " 'SOLDIER #2: Oh, yeah, I agree with that.',\n",
    " 'ARTHUR: Will you ask your master if he wants to join my court at Camelot?!',\n",
    " 'SOLDIER #1: But then of course a-- African swallows are non-migratory.',\n",
    " 'SOLDIER #2: Oh, yeah...',\n",
    " \"SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!\",\n",
    " 'Supposing two swallows carried it together?',\n",
    " \"SOLDIER #1: No, they'd have to have it on a line.\",\n",
    " 'SOLDIER #2: Well, simple!',\n",
    " \"They'd just use a strand of creeper!\",\n",
    " 'SOLDIER #1: What, held under the dorsal guiding feathers?',\n",
    " 'SOLDIER #2: Well, why not?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!',\n",
       " '[clop clop clop] \\nSOLDIER #1: Halt!',\n",
       " 'Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.',\n",
       " 'King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!',\n",
       " 'ARTHUR: I am, ...  and this is my trusty servant Patsy.',\n",
       " 'We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.',\n",
       " 'I must speak with your lord and master.',\n",
       " 'SOLDIER #1: What?',\n",
       " 'Ridden on a horse?',\n",
       " 'ARTHUR: Yes!',\n",
       " \"SOLDIER #1: You're using coconuts!\",\n",
       " 'ARTHUR: What?',\n",
       " \"SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\",\n",
       " 'ARTHUR: So?',\n",
       " \"We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\",\n",
       " 'ARTHUR: We found them.',\n",
       " 'SOLDIER #1: Found them?',\n",
       " 'In Mercea?',\n",
       " \"The coconut's tropical!\",\n",
       " 'ARTHUR: What do you mean?',\n",
       " 'SOLDIER #1: Well, this is a temperate zone.',\n",
       " 'ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?',\n",
       " 'SOLDIER #1: Are you suggesting coconuts migrate?',\n",
       " 'ARTHUR: Not at all.',\n",
       " 'They could be carried.',\n",
       " 'SOLDIER #1: What?',\n",
       " 'A swallow carrying a coconut?',\n",
       " 'ARTHUR: It could grip it by the husk!',\n",
       " \"SOLDIER #1: It's not a question of where he grips it!\",\n",
       " \"It's a simple question of weight ratios!\",\n",
       " 'A five ounce bird could not carry a one pound coconut.',\n",
       " \"ARTHUR: Well, it doesn't matter.\",\n",
       " 'Will you go and tell your master that Arthur from the Court of Camelot is here.',\n",
       " 'SOLDIER #1: Listen.',\n",
       " 'In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?',\n",
       " 'ARTHUR: Please!',\n",
       " 'SOLDIER #1: Am I right?',\n",
       " \"ARTHUR: I'm not interested!\",\n",
       " 'SOLDIER #2: It could be carried by an African swallow!',\n",
       " 'SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.',\n",
       " \"That's my point.\",\n",
       " 'SOLDIER #2: Oh, yeah, I agree with that.',\n",
       " 'ARTHUR: Will you ask your master if he wants to join my court at Camelot?!',\n",
       " 'SOLDIER #1: But then of course a-- African swallows are non-migratory.',\n",
       " 'SOLDIER #2: Oh, yeah...',\n",
       " \"SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!\",\n",
       " 'Supposing two swallows carried it together?',\n",
       " \"SOLDIER #1: No, they'd have to have it on a line.\",\n",
       " 'SOLDIER #2: Well, simple!',\n",
       " \"They'd just use a strand of creeper!\",\n",
       " 'SOLDIER #1: What, held under the dorsal guiding feathers?',\n",
       " 'SOLDIER #2: Well, why not?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with re.search()\n",
    "\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.\n",
    "\n",
    "Instructions\n",
    " - Print the start and end indexes for the first occurence of the word \"coconuts\" in scene_one. To do this, first use re.search() to define match, and then use the .start() and .end() methods of match.\n",
    " - Write a regular expression called pattern1 to find anything in square brackets.\n",
    " - Use re.search() with the previous pattern to find the first text in square brackets in the scene.\n",
    " - Use re.match() to match the script notation in the fourth line (ARTHUR:) and print the result. The tokenized sentences of scene_one are available in your namespace as sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<_sre.SRE_Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "re.search(pattern1, scene_one)\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hint\n",
    "To create match, use re.search() with the arguments of interest - the word, followed by the scene. Once you've created match, you can use its  .start() and .end() methods inside a call to  print() to print the start and end indexes.\n",
    "To create pattern1, use r\"\\[.*\\]\". You can then use it with scene_one in a call to re.search() to find the first text in square brackets.\n",
    "To create pattern2, use r\"[\\w\\s]+:\". Then, you can use it with the fourth sentence (sentences[3]) in a call to re.match()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced tokenization with NLTK and regex - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choosing a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a tokenizer\n",
    "50xp\n",
    "Given the following string, which is the best tokenizer? If possible, we want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "\n",
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "Possible Answers\n",
    "- r\"\\w+(\\?!)\"\n",
    "- r\"(\\w+|#\\d|\\?|!)\"\n",
    "- r\"(#\\d\\w+\\?!)\" (Correct)\n",
    "- r\"\\s+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"(#\\d\\w+\\?!)\", my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex with NLTK tokenization\n",
    "\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n",
    "\n",
    "Remember: | is like an \"or\" statement in regex and square brackets can be used to create groups.\n",
    "\n",
    "Instructions\n",
    " - Import the regexp_tokenize and TweetTokenizer from nltk.tokenize.\n",
    " - Define a regex pattern called pattern1 to match hashtags. A hashtag is something like #nlp. Then, call regexp_tokenize() with your new hashtag pattern on the first tweet in tweets.\n",
    " - Write a new pattern called pattern2 to match mentions and hashtags. A mention is something like @DataCamp. Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets. You can access the last element of a list using -1 as the index.\n",
    " - Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize each tweet into a new list called all_tokens. To do this, use the .tokenize() method of tknzr, with t as your iterator variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets=['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the best #nlp exercise ive found online! #python',\n",
       " '#NLP is super fun! <3 #learning',\n",
       " 'Thanks @datacamp :) #nlp #python']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0],pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([#|@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1],pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#nlp', '#python']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0],pattern1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@datacamp', '#nlp', '#python']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([#|@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1],pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  '#nlp',\n",
       "  'exercise',\n",
       "  'ive',\n",
       "  'found',\n",
       "  'online',\n",
       "  '!',\n",
       "  '#python'],\n",
       " ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'],\n",
       " ['Thanks', '@datacamp', ':)', '#nlp', '#python']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint\n",
    "You can import x from y using the command from y import x.\n",
    "To create pattern1, use r\"#\\w+\". Then, pass it in as the second argument to regexp_tokenize(), where the first argument is tweets[0].\n",
    "To create pattern2, use r\"([#|@]\\w+)\". After this, you can use it with regexp_tokenize() as you did above.\n",
    "Use TweetTokenizer to create tknzr. Then, in the output expression of your list comprehension, use the .tokenize() method of tknzr. Be sure to use t as your iterator variable and tweets as your iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Other External Examples : \n",
    "tknzr = TweetTokenizer()\n",
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ascii tokenization\n",
    "\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called german_text. Explore it in the Shell and notice the German characters and emoji!\n",
    "\n",
    "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF').\n",
    "\n",
    "Instructions\n",
    " - Tokenize all the words in german_text.\n",
    " - Tokenize only the capital words in german_text.\n",
    " - First, write a pattern called capital_words to match only capital words.\n",
    " - Then, tokenize it using regexp_tokenize(). Make sure to check for the German Ü!\n",
    " - Tokenize only the emoji in german_text. To write the pattern, refer to the unicode ranges for emoji given in the assignment text, using | to separate them. You can then tokenize it using regexp_tokenize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_text='Wann gehen wir zum Pizza? 🍕 Und fährst du mit Über? 🚕'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint\n",
    "You can use word_tokenize() to tokenize the words in german_text.\n",
    "The pattern to match only capital words is r\"[A-ZÜ]\\w+\".\n",
    "    To write the pattern to match emoji, separate the unicode ranges for emoji shown in the assignment text using |. After writing the patterns, be sure to tokenize using regexp_tokenize() and then print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words in german_text\n",
    "word_tokenize(german_text)\n",
    "\n",
    "# Tokenize only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple topic identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Word counts with bag-of-words - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words picker\n",
    "\n",
    "It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text?\n",
    "\n",
    "\"The cat is in the box. The cat box.\"\n",
    "\n",
    "Possible Answers\n",
    "- ('the', 3), ('box.', 2), ('cat', 2), ('is', 1)\n",
    "- ('The', 3), ('box', 2), ('cat', 2), ('is', 1), ('in', 1), ('.', 1)\n",
    "- ('the', 3), ('cat box', 1), ('cat', 1), ('box', 1), ('is', 1), ('in', 1)\n",
    "- ('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1) (Correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article = pd.read_table(\"article.txt\")\n",
    "article = str(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Counter with bag-of-words\n",
    "\n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "word_tokenize has been imported for you.\n",
    "\n",
    "Instructions\n",
    "- Import Counter from collections.\n",
    "- Use word_tokenize() to split the article into tokens.\n",
    "- Use a list comprehension with t as the iterator variable to convert all the tokens into lowercase. The .lower() method converts text into lowercase.\n",
    "- Create a bag-of-words counter called bow_simple by using Counter() with lower_tokens as the argument.\n",
    "- Use the .most_common() method of bow_simple to print the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 147), ('of', 81), ('.', 70), ('to', 61), ('a', 59), (\"''\", 42), ('and', 41), ('in', 41), ('(', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text preprocessing - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing steps\n",
    "\n",
    "Which of the following are useful text preprocessing steps?\n",
    "\n",
    "Possible Answers\n",
    "- Stems, spelling corrections, lowercase.\n",
    "- Lemmatization, lowercasing, removing unwanted tokens. (Correct)\n",
    "- Removing stop words, leaving in capital words.\n",
    "- Strip stop words, word endings and digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing practice\n",
    "\n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "\n",
    "You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported.\n",
    "\n",
    "Instructions\n",
    "- Import the WordNetLemmatizer class from nltk.stem.\n",
    "- Create a list called alpha_only that iterates through lower_tokens and retains only alphabetical characters. You can use the .isalpha() method to check for this.\n",
    "- Create another list called no_stops in which you remove all stop words, which are held in a list called english_stops.\n",
    "- Initialize a WordNetLemmatizer object called wordnet_lemmatizer and use its .lemmatize() method on the tokens in no_stops to create a new list called lemmatized.\n",
    "- Finally, create a new Counter called bow with the lemmatized words and show the 10 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stops = pd.read_table(\"english_stops.txt\")\n",
    "english_stops = [english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-79e008beda61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mno_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_only\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-79e008beda61>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mno_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_only\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mohammads6\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m    952\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "no_stops = [t for t in alpha_only if t not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a81c6e46eb62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Remove all stop words: no_stops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mno_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_only\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Instantiate the WordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-a81c6e46eb62>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Remove all stop words: no_stops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mno_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_only\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Instantiate the WordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mohammads6\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m    952\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-876b36a0bff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove all stop words: no_stops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mno_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_only\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-876b36a0bff5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove all stop words: no_stops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mno_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_only\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mohammads6\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m    952\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empty',\n",
       " 'dataframe',\n",
       " 'columns',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'finding',\n",
       " 'and',\n",
       " 'resolving',\n",
       " 'of',\n",
       " 'defects',\n",
       " 'that',\n",
       " 'prevent',\n",
       " 'correct',\n",
       " 'operation',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'software',\n",
       " 'or',\n",
       " 'a',\n",
       " 'system',\n",
       " 'books',\n",
       " 'have',\n",
       " 'been',\n",
       " 'written',\n",
       " 'about',\n",
       " 'debugging',\n",
       " 'see',\n",
       " 'below',\n",
       " 'further',\n",
       " 'reading',\n",
       " 'as',\n",
       " 'it',\n",
       " 'involves',\n",
       " 'numerous',\n",
       " 'aspects',\n",
       " 'including',\n",
       " 'interactive',\n",
       " 'debugging',\n",
       " 'control',\n",
       " 'flow',\n",
       " 'integration',\n",
       " 'testing',\n",
       " 'files',\n",
       " 'monitoring',\n",
       " 'application',\n",
       " 'system',\n",
       " 'memory',\n",
       " 'dumps',\n",
       " 'profiling',\n",
       " 'computer',\n",
       " 'programming',\n",
       " 'statistical',\n",
       " 'process',\n",
       " 'control',\n",
       " 'and',\n",
       " 'special',\n",
       " 'design',\n",
       " 'tactics',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'detection',\n",
       " 'while',\n",
       " 'simplifying',\n",
       " 'computer',\n",
       " 'log',\n",
       " 'entry',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mark',\n",
       " 'nbsp',\n",
       " 'ii',\n",
       " 'with',\n",
       " 'a',\n",
       " 'moth',\n",
       " 'taped',\n",
       " 'to',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'bug',\n",
       " 'and',\n",
       " 'debugging',\n",
       " 'are',\n",
       " 'popularly',\n",
       " 'attributed',\n",
       " 'to',\n",
       " 'admiral',\n",
       " 'grace',\n",
       " 'hopper',\n",
       " 'in',\n",
       " 'the',\n",
       " 'http',\n",
       " 'grace',\n",
       " 'hopper',\n",
       " 'from',\n",
       " 'foldoc',\n",
       " 'while',\n",
       " 'she',\n",
       " 'was',\n",
       " 'working',\n",
       " 'on',\n",
       " 'a',\n",
       " 'harvard',\n",
       " 'mark',\n",
       " 'ii',\n",
       " 'computer',\n",
       " 'at',\n",
       " 'harvard',\n",
       " 'university',\n",
       " 'her',\n",
       " 'associates',\n",
       " 'discovered',\n",
       " 'a',\n",
       " 'moth',\n",
       " 'stuck',\n",
       " 'in',\n",
       " 'a',\n",
       " 'relay',\n",
       " 'and',\n",
       " 'thereby',\n",
       " 'impeding',\n",
       " 'operation',\n",
       " 'whereupon',\n",
       " 'she',\n",
       " 'remarked',\n",
       " 'that',\n",
       " 'they',\n",
       " 'were',\n",
       " 'debugging',\n",
       " 'the',\n",
       " 'system',\n",
       " 'however',\n",
       " 'the',\n",
       " 'term',\n",
       " 'bug',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'technical',\n",
       " 'error',\n",
       " 'dates',\n",
       " 'back',\n",
       " 'at',\n",
       " 'least',\n",
       " 'to',\n",
       " 'and',\n",
       " 'thomas',\n",
       " 'edison',\n",
       " 'see',\n",
       " 'software',\n",
       " 'bug',\n",
       " 'for',\n",
       " 'a',\n",
       " 'full',\n",
       " 'discussion',\n",
       " 'and',\n",
       " 'debugging',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'have',\n",
       " 'been',\n",
       " 'used',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'in',\n",
       " 'aeronautics',\n",
       " 'before',\n",
       " 'entering',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'computers',\n",
       " 'indeed',\n",
       " 'in',\n",
       " 'an',\n",
       " 'interview',\n",
       " 'grace',\n",
       " 'hopper',\n",
       " 'remarked',\n",
       " 'that',\n",
       " 'she',\n",
       " 'was',\n",
       " 'not',\n",
       " 'coining',\n",
       " 'the',\n",
       " 'term',\n",
       " 'citation',\n",
       " 'the',\n",
       " 'moth',\n",
       " 'fit',\n",
       " 'the',\n",
       " 'already',\n",
       " 'existing',\n",
       " 'terminology',\n",
       " 'so',\n",
       " 'it',\n",
       " 'was',\n",
       " 'saved',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'from',\n",
       " 'robert',\n",
       " 'oppenheimer',\n",
       " 'director',\n",
       " 'of',\n",
       " 'the',\n",
       " 'wwii',\n",
       " 'atomic',\n",
       " 'bomb',\n",
       " 'manhattan',\n",
       " 'project',\n",
       " 'at',\n",
       " 'los',\n",
       " 'alamos',\n",
       " 'nm',\n",
       " 'used',\n",
       " 'the',\n",
       " 'term',\n",
       " 'in',\n",
       " 'a',\n",
       " 'letter',\n",
       " 'to',\n",
       " 'ernest',\n",
       " 'lawrence',\n",
       " 'at',\n",
       " 'uc',\n",
       " 'berkeley',\n",
       " 'dated',\n",
       " 'october',\n",
       " 'http',\n",
       " 'regarding',\n",
       " 'the',\n",
       " 'recruitment',\n",
       " 'of',\n",
       " 'additional',\n",
       " 'technical',\n",
       " 'oxford',\n",
       " 'english',\n",
       " 'dictionary',\n",
       " 'entry',\n",
       " 'for',\n",
       " 'debug',\n",
       " 'quotes',\n",
       " 'the',\n",
       " 'term',\n",
       " 'debugging',\n",
       " 'used',\n",
       " 'in',\n",
       " 'reference',\n",
       " 'to',\n",
       " 'airplane',\n",
       " 'engine',\n",
       " 'testing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'article',\n",
       " 'in',\n",
       " 'the',\n",
       " 'journal',\n",
       " 'of',\n",
       " 'the',\n",
       " 'royal',\n",
       " 'aeronautical',\n",
       " 'society',\n",
       " 'an',\n",
       " 'article',\n",
       " 'in',\n",
       " 'airforce',\n",
       " 'june',\n",
       " 'nbsp',\n",
       " 'also',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'debugging',\n",
       " 'this',\n",
       " 'time',\n",
       " 'of',\n",
       " 'aircraft',\n",
       " 'cameras',\n",
       " 'computer',\n",
       " 'was',\n",
       " 'found',\n",
       " 'on',\n",
       " 'september',\n",
       " 'the',\n",
       " 'term',\n",
       " 'was',\n",
       " 'not',\n",
       " 'adopted',\n",
       " 'by',\n",
       " 'computer',\n",
       " 'programmers',\n",
       " 'until',\n",
       " 'the',\n",
       " 'early',\n",
       " 'seminal',\n",
       " 'article',\n",
       " 'by',\n",
       " 'gills',\n",
       " 'gill',\n",
       " 'http',\n",
       " 'the',\n",
       " 'diagnosis',\n",
       " 'of',\n",
       " 'mistakes',\n",
       " 'in',\n",
       " 'programmes',\n",
       " 'on',\n",
       " 'the',\n",
       " 'edsac',\n",
       " 'proceedings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'royal',\n",
       " 'society',\n",
       " 'of',\n",
       " 'london',\n",
       " 'series',\n",
       " 'a',\n",
       " 'mathematical',\n",
       " 'and',\n",
       " 'physical',\n",
       " 'sciences',\n",
       " 'vol',\n",
       " 'no',\n",
       " 'may',\n",
       " 'pp',\n",
       " 'in',\n",
       " 'is',\n",
       " 'the',\n",
       " 'earliest',\n",
       " 'discussion',\n",
       " 'of',\n",
       " 'programming',\n",
       " 'errors',\n",
       " 'but',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'use',\n",
       " 'the',\n",
       " 'term',\n",
       " 'bug',\n",
       " 'or',\n",
       " 'debugging',\n",
       " 'the',\n",
       " 'association',\n",
       " 'for',\n",
       " 'computing',\n",
       " 'digital',\n",
       " 'library',\n",
       " 'the',\n",
       " 'term',\n",
       " 'debugging',\n",
       " 'is',\n",
       " 'first',\n",
       " 'used',\n",
       " 'in',\n",
       " 'three',\n",
       " 'papers',\n",
       " 'from',\n",
       " 'acm',\n",
       " 'national',\n",
       " 'campbell',\n",
       " 'http',\n",
       " 'evolution',\n",
       " 'of',\n",
       " 'automatic',\n",
       " 'computation',\n",
       " 'proceedings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'acm',\n",
       " 'national',\n",
       " 'meeting',\n",
       " 'pittsburgh',\n",
       " 'p',\n",
       " 'orden',\n",
       " 'http',\n",
       " 'solution',\n",
       " 'of',\n",
       " 'systems',\n",
       " 'of',\n",
       " 'linear',\n",
       " 'inequalities',\n",
       " 'on',\n",
       " 'a',\n",
       " 'digital',\n",
       " 'computer',\n",
       " 'proceedings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'acm',\n",
       " 'national',\n",
       " 'meeting',\n",
       " 'pittsburgh',\n",
       " 'demuth',\n",
       " 'john',\n",
       " 'jackson',\n",
       " 'edmund',\n",
       " 'klein',\n",
       " 'metropolis',\n",
       " 'walter',\n",
       " 'orvedahl',\n",
       " 'james',\n",
       " 'richardson',\n",
       " 'http',\n",
       " 'maniac',\n",
       " 'proceedings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'acm',\n",
       " 'national',\n",
       " 'meeting',\n",
       " 'toronto',\n",
       " 'two',\n",
       " 'of',\n",
       " 'the',\n",
       " 'three',\n",
       " 'use',\n",
       " 'the',\n",
       " 'term',\n",
       " 'in',\n",
       " 'quotation',\n",
       " 'debugging',\n",
       " 'was',\n",
       " 'a',\n",
       " 'common',\n",
       " 'enough',\n",
       " 'term',\n",
       " 'to',\n",
       " 'be',\n",
       " 'mentioned',\n",
       " 'in',\n",
       " 'passing',\n",
       " 'without',\n",
       " 'explanation',\n",
       " 'on',\n",
       " 'page',\n",
       " 'of',\n",
       " 'the',\n",
       " 'compatible',\n",
       " 'manual',\n",
       " 'http',\n",
       " 'the',\n",
       " 'compatible',\n",
       " 'system',\n",
       " 'press',\n",
       " 'article',\n",
       " 'the',\n",
       " 'elusive',\n",
       " 'computer',\n",
       " 'aldrich',\n",
       " 'kidwell',\n",
       " 'http',\n",
       " 'stalking',\n",
       " 'the',\n",
       " 'elusive',\n",
       " 'computer',\n",
       " 'bug',\n",
       " 'ieee',\n",
       " 'annals',\n",
       " 'of',\n",
       " 'the',\n",
       " 'history',\n",
       " 'of',\n",
       " 'computing',\n",
       " 'discusses',\n",
       " 'the',\n",
       " 'etymology',\n",
       " 'of',\n",
       " 'bug',\n",
       " 'and',\n",
       " 'debug',\n",
       " 'in',\n",
       " 'greater',\n",
       " 'software',\n",
       " 'and',\n",
       " 'electronic',\n",
       " 'systems',\n",
       " 'have',\n",
       " 'become',\n",
       " 'generally',\n",
       " 'more',\n",
       " 'complex',\n",
       " 'the',\n",
       " 'various',\n",
       " 'common',\n",
       " 'debugging',\n",
       " 'techniques',\n",
       " 'have',\n",
       " 'expanded',\n",
       " 'with',\n",
       " 'more',\n",
       " 'methods',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'anomalies',\n",
       " 'assess',\n",
       " 'impact',\n",
       " 'and',\n",
       " 'schedule',\n",
       " 'software',\n",
       " 'patches',\n",
       " 'or',\n",
       " 'full',\n",
       " 'updates',\n",
       " 'to',\n",
       " 'a',\n",
       " 'system',\n",
       " 'the',\n",
       " 'words',\n",
       " 'anomaly',\n",
       " 'and',\n",
       " 'discrepancy',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'as',\n",
       " 'being',\n",
       " 'more',\n",
       " 'neutral',\n",
       " 'terms',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'the',\n",
       " 'words',\n",
       " 'error',\n",
       " 'and',\n",
       " 'defect',\n",
       " 'or',\n",
       " 'bug',\n",
       " 'where',\n",
       " 'there',\n",
       " 'might',\n",
       " 'be',\n",
       " 'an',\n",
       " 'implication',\n",
       " 'that',\n",
       " 'all',\n",
       " 'or',\n",
       " 'must',\n",
       " 'be',\n",
       " 'fixed',\n",
       " 'at',\n",
       " 'all',\n",
       " 'costs',\n",
       " 'instead',\n",
       " 'an',\n",
       " 'impact',\n",
       " 'assessment',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'to',\n",
       " 'determine',\n",
       " 'if',\n",
       " 'changes',\n",
       " 'to',\n",
       " 'remove',\n",
       " 'an',\n",
       " 'or',\n",
       " 'would',\n",
       " 'be',\n",
       " 'for',\n",
       " 'the',\n",
       " 'system',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'a',\n",
       " 'scheduled',\n",
       " 'new',\n",
       " 'release',\n",
       " 'might',\n",
       " 'render',\n",
       " 'the',\n",
       " 'change',\n",
       " 's',\n",
       " 'unnecessary',\n",
       " 'not',\n",
       " 'all',\n",
       " 'issues',\n",
       " 'are',\n",
       " 'or',\n",
       " 'in',\n",
       " 'a',\n",
       " 'system',\n",
       " 'also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'important',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'where',\n",
       " 'a',\n",
       " 'change',\n",
       " 'might',\n",
       " 'be',\n",
       " 'more',\n",
       " 'upsetting',\n",
       " 'to',\n",
       " 'users',\n",
       " 'than',\n",
       " 'living',\n",
       " 'with',\n",
       " 'the',\n",
       " 'known',\n",
       " 'problem',\n",
       " 's',\n",
       " 'where',\n",
       " 'the',\n",
       " 'cure',\n",
       " 'would',\n",
       " 'be',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'the',\n",
       " 'disease',\n",
       " 'basing',\n",
       " 'decisions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'acceptability',\n",
       " 'of',\n",
       " 'some',\n",
       " 'anomalies',\n",
       " 'can',\n",
       " 'avoid',\n",
       " 'a',\n",
       " 'culture',\n",
       " 'of',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'where',\n",
       " 'people',\n",
       " 'might',\n",
       " 'be',\n",
       " 'tempted',\n",
       " 'to',\n",
       " 'deny',\n",
       " 'the',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'problems',\n",
       " 'so',\n",
       " 'that',\n",
       " 'the',\n",
       " 'result',\n",
       " 'would',\n",
       " 'appear',\n",
       " 'as',\n",
       " 'zero',\n",
       " 'considering',\n",
       " 'the',\n",
       " 'collateral',\n",
       " 'issues',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'impact',\n",
       " 'assessment',\n",
       " 'then',\n",
       " 'broader',\n",
       " 'debugging',\n",
       " 'techniques',\n",
       " 'will',\n",
       " 'expand',\n",
       " 'to',\n",
       " 'determine',\n",
       " 'the',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'anomalies',\n",
       " 'how',\n",
       " 'often',\n",
       " 'the',\n",
       " 'same',\n",
       " 'bugs',\n",
       " 'occur',\n",
       " 'to',\n",
       " 'help',\n",
       " 'assess',\n",
       " 'their',\n",
       " 'impact',\n",
       " 'to',\n",
       " 'the',\n",
       " 'overall',\n",
       " 'on',\n",
       " 'video',\n",
       " 'game',\n",
       " 'consoles',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'done',\n",
       " 'with',\n",
       " 'special',\n",
       " 'hardware',\n",
       " 'such',\n",
       " 'as',\n",
       " 'this',\n",
       " 'xbox',\n",
       " 'console',\n",
       " 'debug',\n",
       " 'unit',\n",
       " 'intended',\n",
       " 'for',\n",
       " 'ranges',\n",
       " 'in',\n",
       " 'complexity',\n",
       " 'from',\n",
       " 'fixing',\n",
       " 'simple',\n",
       " 'errors',\n",
       " 'to',\n",
       " 'performing',\n",
       " 'lengthy',\n",
       " 'and',\n",
       " 'tiresome',\n",
       " 'tasks',\n",
       " 'of',\n",
       " 'data',\n",
       " 'collection',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'scheduling',\n",
       " 'updates',\n",
       " 'the',\n",
       " 'debugging',\n",
       " 'skill',\n",
       " 'of',\n",
       " 'the',\n",
       " 'programmer',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'major',\n",
       " 'factor',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'debug',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'but',\n",
       " 'the',\n",
       " 'difficulty',\n",
       " 'of',\n",
       " 'software',\n",
       " 'debugging',\n",
       " 'varies',\n",
       " 'greatly',\n",
       " 'with',\n",
       " 'the',\n",
       " 'complexity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'system',\n",
       " 'and',\n",
       " 'also',\n",
       " 'depends',\n",
       " 'to',\n",
       " 'some',\n",
       " 'extent',\n",
       " 'on',\n",
       " 'the',\n",
       " 'programming',\n",
       " 'language',\n",
       " 's',\n",
       " 'used',\n",
       " 'and',\n",
       " 'the',\n",
       " 'available',\n",
       " 'tools',\n",
       " 'such',\n",
       " 'as',\n",
       " 'debuggers',\n",
       " 'are',\n",
       " 'software',\n",
       " 'tools',\n",
       " 'which',\n",
       " 'enable',\n",
       " 'the',\n",
       " 'programmer',\n",
       " 'to',\n",
       " 'monitor',\n",
       " 'the',\n",
       " 'execution',\n",
       " 'computers',\n",
       " 'of',\n",
       " 'a',\n",
       " 'program',\n",
       " 'stop',\n",
       " 'it',\n",
       " 'restart',\n",
       " 'it',\n",
       " 'set',\n",
       " 'breakpoints',\n",
       " 'and',\n",
       " 'change',\n",
       " 'values',\n",
       " 'in',\n",
       " 'memory',\n",
       " 'the',\n",
       " 'term',\n",
       " 'can',\n",
       " 'also',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'person',\n",
       " 'who',\n",
       " 'is',\n",
       " 'doing',\n",
       " 'the',\n",
       " 'programming',\n",
       " 'languages',\n",
       " 'such',\n",
       " 'as',\n",
       " 'java',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'make',\n",
       " 'debugging',\n",
       " 'easier',\n",
       " 'because',\n",
       " 'they',\n",
       " 'have',\n",
       " 'features',\n",
       " 'such',\n",
       " 'as',\n",
       " 'exception',\n",
       " 'handling',\n",
       " 'that',\n",
       " 'make',\n",
       " 'real',\n",
       " 'sources',\n",
       " 'of',\n",
       " 'erratic',\n",
       " 'behaviour',\n",
       " 'easier',\n",
       " 'to',\n",
       " 'spot',\n",
       " 'in',\n",
       " 'programming',\n",
       " 'languages',\n",
       " 'such',\n",
       " 'as',\n",
       " 'c',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'or',\n",
       " 'assembly',\n",
       " 'bugs',\n",
       " 'may',\n",
       " 'cause',\n",
       " 'silent',\n",
       " 'problems',\n",
       " 'such',\n",
       " 'as',\n",
       " 'memory',\n",
       " 'corruption',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'often',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'see',\n",
       " 'where',\n",
       " 'the',\n",
       " 'initial',\n",
       " 'problem',\n",
       " 'happened',\n",
       " 'in',\n",
       " 'those',\n",
       " 'cases',\n",
       " 'memory',\n",
       " 'debugger',\n",
       " 'tools',\n",
       " 'may',\n",
       " 'be',\n",
       " 'certain',\n",
       " 'situations',\n",
       " 'general',\n",
       " 'purpose',\n",
       " 'software',\n",
       " 'tools',\n",
       " 'that',\n",
       " 'are',\n",
       " 'language',\n",
       " 'specific',\n",
       " 'in',\n",
       " 'nature',\n",
       " 'can',\n",
       " 'be',\n",
       " 'very',\n",
       " 'useful',\n",
       " 'these',\n",
       " 'take',\n",
       " 'the',\n",
       " 'form',\n",
       " 'of',\n",
       " 'of',\n",
       " 'tools',\n",
       " 'for',\n",
       " 'static',\n",
       " 'code',\n",
       " 'code',\n",
       " 'analysis',\n",
       " 'these',\n",
       " 'tools',\n",
       " 'look',\n",
       " 'for',\n",
       " 'a',\n",
       " 'very',\n",
       " 'specific',\n",
       " 'set',\n",
       " 'of',\n",
       " 'known',\n",
       " 'problems',\n",
       " 'some',\n",
       " 'common',\n",
       " 'and',\n",
       " 'some',\n",
       " 'rare',\n",
       " 'within',\n",
       " 'the',\n",
       " 'source',\n",
       " 'code',\n",
       " 'all',\n",
       " 'such',\n",
       " 'issues',\n",
       " 'detected',\n",
       " 'by',\n",
       " 'these',\n",
       " 'tools',\n",
       " 'would',\n",
       " 'rarely',\n",
       " 'be',\n",
       " 'picked',\n",
       " 'up',\n",
       " 'by',\n",
       " 'a',\n",
       " 'compiler',\n",
       " 'or',\n",
       " 'interpreter',\n",
       " 'thus',\n",
       " 'they',\n",
       " 'are',\n",
       " 'not',\n",
       " 'syntax',\n",
       " 'checkers',\n",
       " 'but',\n",
       " 'more',\n",
       " 'semantic',\n",
       " 'checkers',\n",
       " 'some',\n",
       " 'tools',\n",
       " 'claim',\n",
       " 'to',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'unique',\n",
       " 'problems',\n",
       " 'both',\n",
       " 'commercial',\n",
       " 'and',\n",
       " 'free',\n",
       " 'tools',\n",
       " 'exist',\n",
       " 'in',\n",
       " 'various',\n",
       " 'languages',\n",
       " 'these',\n",
       " 'tools',\n",
       " 'can',\n",
       " 'be',\n",
       " 'extremely',\n",
       " 'useful',\n",
       " 'when',\n",
       " 'checking',\n",
       " 'very',\n",
       " 'large',\n",
       " 'source',\n",
       " 'trees',\n",
       " 'where',\n",
       " 'it',\n",
       " 'is',\n",
       " 'impractical',\n",
       " 'to',\n",
       " 'do',\n",
       " 'code',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in alpha_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 8, 10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2,4,6,8,10,12]\n",
    "y = [3,6,9,12]\n",
    "z = [t for t in x if t not in y]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two', 'four', 'eight', 'ten']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [\"two\", \"four\", \"six\", \"eight\", \"ten\", \"twelve\"]\n",
    "y = [\"three\",\"six\", \"nine\", \"twelve\"]\n",
    "[t for t in x if t not in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-881fb53b3f39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-881fb53b3f39>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\mohammads6\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    951\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m    952\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "#[t for t in alpha_only if t not in english_stops]\n",
    "x = alpha_only\n",
    "y = english_stops\n",
    "[t for t in x if t not in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to gensim - Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are word vectors?\n",
    "\n",
    "What are word vectors and how do they help with NLP?\n",
    "\n",
    "Possible Answers\n",
    "Click or Press Ctrl+1 to focus\n",
    " - They are similar to bags of words, just with numbers. You use them to count how many tokens there are.\n",
    " - Word vectors are sparse arrays representing bigrams in the corpora. You can use them to compare two sets of words to one another.\n",
    " - Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus. (Correct)\n",
    " - Word vectors don't actually help NLP and are just hype.\n",
    "Take Hint (-15xp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and querying a corpus with gensim\n",
    "\n",
    "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
    "\n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus.\n",
    "\n",
    "Instructions\n",
    "- Import Dictionary from gensim.corpora.dictionary.\n",
    "- Initialize a gensim Dictionary with the tokens in articles.\n",
    "- Obtain the id for \"computer\" from dictionary. To do this, use its .token2id method which returns ids from text, and then chain .get() which returns tokens from ids. Pass in \"computer\" as an argument to .get().\n",
    "- Use a list comprehension in which you iterate over articles to create a gensim MmCorpus from dictionary.\n",
    "- In the output expression, use the .doc2bow() method on dictionary with article as the argument.\n",
    "- Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8ee46e914ef0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import Dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create a Dictionary from the articles: dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
